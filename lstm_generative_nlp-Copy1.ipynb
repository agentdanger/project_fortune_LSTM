{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import FreqDist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, LSTM, TimeDistributed, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=20000\n",
    "max_len = 350\n",
    "HIDDEN_DIM = 1000\n",
    "BATCH_SIZE = 1000\n",
    "LAYER_NUM = 3\n",
    "EPOCHS = 20\n",
    "GENERATE_LENGTH = 100\n",
    "quote_start_token = 'QUOTE_START'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n"
     ]
    }
   ],
   "source": [
    "print('Reading data...')\n",
    "\n",
    "filename = 'fortune_quotes.csv'\n",
    "data = pd.read_csv(filename)\n",
    "df = pd.DataFrame(data)\n",
    "doc = df['quotes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "Formatting data for analysis...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ebab6b37c4f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0msequences_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m \u001b[0msequences_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquotes_y_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquotes_max_len_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_word_to_ix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquote\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquotes_y_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquote\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "quotes_x = []\n",
    "quotes_y = []\n",
    "\n",
    "print('Creating dictionary...')\n",
    "\n",
    "for quote in doc:\n",
    "    quote = quote.replace('/', '').replace('\\n', '').replace(\n",
    "        '\\r', '').replace('\"', '').replace('[', '').replace(\n",
    "        ']', '').replace('“', '').replace('”', '').replace(\n",
    "        '.', '').replace('?', '').replace(':', '').replace(\n",
    "        '(', '').replace(')', '').replace(',', '').replace(\n",
    "        '-', '').replace(';', '').replace('!', '')\n",
    "    quote = quote.lower()\n",
    "    quote = '{} {}'.format(quote_start_token, quote)\n",
    "    quote = text_to_word_sequence(quote, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
    "    quotes_x.append(np.copy(quote))\n",
    "    quotes_y.append(np.copy(quote))\n",
    "\n",
    "distribution_x = FreqDist(np.hstack(quotes_x))\n",
    "quotes_vocab_x = distribution_x.most_common(vocab_size-1)\n",
    "\n",
    "print('Formatting data for analysis...')\n",
    "\n",
    "x_ix_to_word = [word[0] for word in quotes_vocab_x]\n",
    "x_ix_to_word.insert(0, 'ZERO_TOKEN')\n",
    "x_ix_to_word.append('UNKNOWN_TOKEN')\n",
    "\n",
    "x_word_to_ix = {word:ix for ix, word in enumerate(x_ix_to_word)}\n",
    "\n",
    "for i, quote in enumerate(quotes_x):\n",
    "    for j, word in enumerate(quote):\n",
    "        if word in x_word_to_ix:\n",
    "            quotes_x[i][j] = x_word_to_ix[word]\n",
    "        else:\n",
    "            quotes_x[i][j] = x_word_to_ix['UNKNOWN_TOKEN']\n",
    "\n",
    "distribution_y = FreqDist(np.hstack(quotes_y))\n",
    "quotes_vocab_y = distribution_y.most_common(vocab_size-1)\n",
    "\n",
    "y_ix_to_word = [word[0] for word in quotes_vocab_y]\n",
    "y_ix_to_word.insert(0, 'ZERO_TOKEN')\n",
    "y_ix_to_word.append('UNKNOWN_TOKEN')\n",
    "\n",
    "y_word_to_ix = {word:ix for ix, word in enumerate(y_ix_to_word)}\n",
    "\n",
    "for x, quote in enumerate(quotes_y):\n",
    "    for y, word in enumerate(quote):\n",
    "        if word in y_word_to_ix:\n",
    "            quotes_y[x][y] = y_word_to_ix[word]\n",
    "        else:\n",
    "            quotes_y[x][y] = y_word_to_ix['UNKNOWN_TOKEN']  \n",
    "            \n",
    "quotes_max_len_x = max([len(quote) for quote in quotes_x])\n",
    "quotes_max_len_y = max([len(quote) for quote in quotes_y])\n",
    "\n",
    "quotes_y_data = []\n",
    "for m in quotes_y:\n",
    "    quotes_y_data.append(np.delete(m, 0))\n",
    "\n",
    "quotes_x_data = pad_sequences(quotes_x, maxlen=quotes_max_len_x,\n",
    "                         dtype='int64', padding='post')\n",
    "quotes_y_data = pad_sequences(quotes_y_data, maxlen=quotes_max_len_y,\n",
    "                              dtype='int64', padding='post')\n",
    "\n",
    "sequences_x = np.zeros((len(quotes_x_data), quotes_max_len_x, len(x_word_to_ix)))\n",
    "for i, quote in enumerate(quotes_x_data):\n",
    "    for j, word in enumerate(quote):\n",
    "        sequences_x[i, j, word] = 1.\n",
    "\n",
    "sequences_y = np.zeros((len(quotes_y_data), quotes_max_len_y, len(y_word_to_ix)))\n",
    "for x, quote in enumerate(quotes_y_data):\n",
    "    for y, word in enumerate(quote):\n",
    "        sequences_y[x, y, word] = 1.\n",
    "            \n",
    "print('Finished!  Parsed, cleaned and formatted \\\n",
    "{:,} quotes and {:,} unique words for analysis.'.format(\n",
    "    len(quotes_x_data), len(x_word_to_ix),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, len(x_word_to_ix)), return_sequences=True))\n",
    "model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(len(x_word_to_ix))))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])\n",
    "print('\\n')\n",
    "print('Model Ready for Training...')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Training started...\")\n",
    "print('\\n')\n",
    "for i in range(EPOCHS):\n",
    "    print('INFO - Training model: Epoch: ', i+1, ' / ', EPOCHS)\n",
    "    model.fit(sequences_x,\n",
    "              sequences_y,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=1,\n",
    "              verbose=1,\n",
    "              shuffle=False)\n",
    "    if i % 10 == 0:\n",
    "        model.save_weights('checkpoint_{}_epoch_{}.hdf5'.format(HIDDEN_DIM, i))\n",
    "\n",
    "print('Training Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
