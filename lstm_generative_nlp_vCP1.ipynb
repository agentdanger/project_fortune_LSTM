{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import FreqDist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, LSTM, TimeDistributed, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=20000\n",
    "max_len = 350\n",
    "HIDDEN_DIM = 1000\n",
    "BATCH_SIZE = 20\n",
    "LAYER_NUM = 3\n",
    "EPOCHS = 1\n",
    "GENERATE_LENGTH = 30\n",
    "quote_start_token = 'QUOTE_START'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n"
     ]
    }
   ],
   "source": [
    "print('Reading data...')\n",
    "\n",
    "filename = 'fortune_quotes.csv'\n",
    "data = pd.read_csv(filename)\n",
    "df = pd.DataFrame(data)\n",
    "doc = df['quotes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "Formatting data for analysis...\n",
      "Finished!  Parsed, cleaned and formatted 6,517 quotes and 13,480 unique words for analysis.\n"
     ]
    }
   ],
   "source": [
    "quotes_x = []\n",
    "quotes_y = []\n",
    "\n",
    "print('Creating dictionary...')\n",
    "\n",
    "for quote in doc:\n",
    "    quote = quote.replace('/', '').replace('\\n', '').replace(\n",
    "        '\\r', '').replace('\"', '').replace('[', '').replace(\n",
    "        ']', '').replace('“', '').replace('”', '').replace(\n",
    "        ':', '').replace('(', '').replace(')', '').replace(\n",
    "        ',', '').replace('-', '').replace(';', '')\n",
    "    quote = quote.lower()\n",
    "    quote = '{} {}'.format(quote_start_token, quote)\n",
    "    quote = text_to_word_sequence(quote, filters='\"#$%&()*+,-/:;<=>@[\\\\]^`{|}~\\t\\n')\n",
    "    quotes_x.append(np.copy(quote))\n",
    "    quotes_y.append(np.copy(quote))\n",
    "\n",
    "distribution_x = FreqDist(np.hstack(quotes_x))\n",
    "quotes_vocab_x = distribution_x.most_common(vocab_size-1)\n",
    "\n",
    "print('Formatting data for analysis...')\n",
    "\n",
    "x_ix_to_word = [word[0] for word in quotes_vocab_x]\n",
    "x_ix_to_word.insert(0, 'ZERO_TOKEN')\n",
    "x_ix_to_word.append('UNKNOWN_TOKEN')\n",
    "\n",
    "x_word_to_ix = {word:ix for ix, word in enumerate(x_ix_to_word)}\n",
    "\n",
    "for i, quote in enumerate(quotes_x):\n",
    "    for j, word in enumerate(quote):\n",
    "        if word in x_word_to_ix:\n",
    "            quotes_x[i][j] = x_word_to_ix[word]\n",
    "        else:\n",
    "            quotes_x[i][j] = x_word_to_ix['UNKNOWN_TOKEN']\n",
    "\n",
    "distribution_y = FreqDist(np.hstack(quotes_y))\n",
    "quotes_vocab_y = distribution_y.most_common(vocab_size-1)\n",
    "\n",
    "y_ix_to_word = [word[0] for word in quotes_vocab_y]\n",
    "y_ix_to_word.insert(0, 'ZERO_TOKEN')\n",
    "y_ix_to_word.append('UNKNOWN_TOKEN')\n",
    "\n",
    "y_word_to_ix = {word:ix for ix, word in enumerate(y_ix_to_word)}\n",
    "\n",
    "for x, quote in enumerate(quotes_y):\n",
    "    for y, word in enumerate(quote):\n",
    "        if word in y_word_to_ix:\n",
    "            quotes_y[x][y] = y_word_to_ix[word]\n",
    "        else:\n",
    "            quotes_y[x][y] = y_word_to_ix['UNKNOWN_TOKEN']  \n",
    "            \n",
    "quotes_max_len_x = max([len(quote) for quote in quotes_x])\n",
    "quotes_max_len_y = max([len(quote) for quote in quotes_y])\n",
    "\n",
    "quotes_y_data = []\n",
    "for m in quotes_y:\n",
    "    quotes_y_data.append(np.delete(m, 0))\n",
    "\n",
    "quotes_x_data = pad_sequences(quotes_x, maxlen=quotes_max_len_x,\n",
    "                         dtype='int64', padding='post')\n",
    "quotes_y_data = pad_sequences(quotes_y_data, maxlen=quotes_max_len_y,\n",
    "                              dtype='int64', padding='post')\n",
    "\n",
    "sequences_x = np.zeros((len(quotes_x_data), quotes_max_len_x, len(x_word_to_ix)))\n",
    "for i, quote in enumerate(quotes_x_data):\n",
    "    for j, word in enumerate(quote):\n",
    "        sequences_x[i, j, word] = 1.\n",
    "\n",
    "sequences_y = np.zeros((len(quotes_y_data), quotes_max_len_y, len(y_word_to_ix)))\n",
    "for x, quote in enumerate(quotes_y_data):\n",
    "    for y, word in enumerate(quote):\n",
    "        sequences_y[x, y, word] = 1.\n",
    "            \n",
    "print('Finished!  Parsed, cleaned and formatted \\\n",
    "{:,} quotes and {:,} unique words for analysis.'.format(\n",
    "    len(quotes_x_data), len(x_word_to_ix),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing data structure\n",
    "# sequence_train = np.zeros((1, 1, len(x_word_to_ix)))\n",
    "# start_ix = np.array([[2]]) # hard coded - needs elegant way of looking up quote_start\n",
    "# for i, quote in enumerate(start_ix):\n",
    "#     for j, word in enumerate(quote):\n",
    "#         sequence_train[i, j, word] = 1.\n",
    "\n",
    "# sequence_test = np.zeros((1, 1, len(x_word_to_ix)))\n",
    "# test_ix = np.array([[0]]) # hard coded - needs elegant way of looking up quote_start\n",
    "# for i, quote in enumerate(test_ix):\n",
    "#     for j, word in enumerate(quote):\n",
    "#         sequence_test[i, j, word] = 1.\n",
    "        \n",
    "# # print(sequence_train)\n",
    "# # print(sequence_test)\n",
    "\n",
    "# test = np.insert(sequence_train, 1, sequence_test, axis=1)\n",
    "\n",
    "# print(\"all\")\n",
    "# print(test)\n",
    "# print(\"only one\")\n",
    "# print(test[:,1])\n",
    "# print(\"add one\")\n",
    "# test = np.insert(test, 2, test[:,1], axis=1)\n",
    "# print(\"all again plus 1\")\n",
    "# print(test)\n",
    "\n",
    "\n",
    "# doc_test, quotes_test, words_test = np.nonzero(test)\n",
    "# print(words_test)\n",
    "# words_generated = []\n",
    "# for x in words_test:\n",
    "#     words_generated = np.append(words_generated, x_ix_to_word[x])\n",
    "# print(words_generated)\n",
    "# print(\" \".join(word for word in words_generated))\n",
    "\n",
    "def generate_text_train(model, GENERATE_LENGTH):\n",
    "    print('Generating sample text...')\n",
    "    sequence_train = np.zeros((1, 1, len(x_word_to_ix)))\n",
    "    start_ix = np.array([[2]]) # hard coded - needs elegant way of looking up quote_start\n",
    "    for i, quote in enumerate(start_ix):\n",
    "        for j, word in enumerate(quote):\n",
    "            sequence_train[i, j, word] = 1.\n",
    "    for n in range(GENERATE_LENGTH):\n",
    "        sequence_train = np.insert(\n",
    "            sequence_train, n, model.predict(sequence_train[:,n-1]), axis=1)\n",
    "    doc_train, quote_train, words_train = np.nonzero(sequence_train)\n",
    "    words_generated = []\n",
    "    for x in words_train:\n",
    "        words_generated = np.append(words_generated, x_ix_to_word[x])\n",
    "    print(\" \".join(word for word in words_generated))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model Ready for Training...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, len(x_word_to_ix)), return_sequences=True))\n",
    "model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(len(x_word_to_ix))))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])\n",
    "print('\\n')\n",
    "print('Model Ready for Training...')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Training started...\")\n",
    "print('\\n')\n",
    "for i in range(EPOCHS):\n",
    "    print('INFO - Training model: Epoch: ', i+1, ' / ', EPOCHS)\n",
    "    model.fit(sequences_x,\n",
    "              sequences_y,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=1,\n",
    "              verbose=1,\n",
    "              shuffle=False)\n",
    "    generate_text_train(model, GENERATE_LENGTH)\n",
    "    if i % 10 == 0:\n",
    "        model.save_weights('checkpoint_{}_epoch_{}.hdf5'.format(HIDDEN_DIM, i))\n",
    "\n",
    "print('Training Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
